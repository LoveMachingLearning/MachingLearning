Python+tensorflow学习
准备开始识别手写数字04

今天继续尝试准备做入门级的人工智能——识别手写数字。今天的任务主要是加深认识，对每一行代码进行理解。

今天学习参照的书是：
《TensorFlow：实战Google深度学习框架（第二版）》
（这本书所使用的Python版本是2.7版本）

对于没有高等数学基础的学习者来说，想初步一窥深度神经网络的一鳞半爪都是那么困难，我今天的学习感受是这世界上似乎还不存在可以完全不讲高等数学知识的深度学习的书籍与博文。
随便翻开一篇博文或一本有关深度神经网络的书，无不看见那些密密麻麻的公式，离开学校二十多年，中等数学所剩下的东西都与眼睛中的沙子一样多了，那些深涩难懂的公式成为横亘在学习道路上的一座座怪物式的大山，似乎我再难以前进一步……
可是——我天生傲骨，岂能认输！
一把年纪，重拾数学是真没有时间了——一个上有老下有小的年纪，却并没有财务自由以可以做自己想做的事情——这已是无法实现的梦想，但好在有无数大佬先贤安慰（也许真的只是安慰）——如果不想很深入了解tensorflow的底层，可以将这些实现的底层数学知识看作一个黑箱不去理会就行了，只要知道什么时候应当用什么对应的函数就好。
这多少给了一点生的希望，于是坚持上路吧，爱好的理想在远方，能走到多远，就是多远，有希望就是幸福的。

【感知机】
神经网络受人类大脑的启发，也被称为连接模型。像人脑一样，神经网络是大量被称为权重的突触相互连接的人造神经元的集合。
我是我能找到的唯一没有涉及到任何数学描述的最基本的【人工神经元】（也叫着朴素感知机）——感知机的描述。
感知机接收多个输入信号，输出一个信号。
这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。
像电流流过导线，向前方输送 电子一样，感知机的信号也会形成流，向前方输送信息。
但是，和实际的电 流不同的是，感知机的信号只有“流/不流”（1/0）两种取值。
0 对应“不传递信号”，1对应“传递信号”。
x 1 、x 2 是输入信号，
y 是输出信号，
w 1 、w 2 是权重 （w 是 weight 的首字母）。
图中的○称为“神经元”或者“节点”（这和人脑的神经元非常相似）。
 
当信号x1,x2…xn传递过来时，每个信号有其在信号可信度上（重要程度上）的权重
【权重w】(有的情况下，w使用大写的W)相当于电流里的电阻。电阻是决定电流流动难度的参数， 电阻越低，通过的电流就越大。
而感知机的权重则是值越大，通过的信号就越大。
不管是电阻还是权重，在控制信号流动难度（或者流 动容易度）这一点上的作用都是一样的。
将每个信号乘以各自的权重，然后再输入给神经元y，然后，就会通过最简单的将每个输入值与权重值相乘后的积进行求和运算，得到结果输出，结果输出往往是这样的：
如果
x1w1+x2w2+……xnwn>θ
θ在这儿表示一个【阈值】，如果求得的值大于这个阈值，是神经元y就会被激活，y这个神经元将输出【1】(感知机的最初模式是这样的，当然实际可以输出实际的一些数据。
如果
x1w1+x2w2+……xnwn<=θ
则y这个神经元不会被激活，它将不会再发出信息给它后面的神经元了。此时可以理解为它输出【0】

我的理解是，这样一来，一个神经元在收到其上游的多个神经元发来的信号（值）后，本身能否被激活就完全取决于阈值θ如何设置以及各个信号值的大小与各个信号对应的权重如何了。

不过后来，人们为感知机引入了【偏置量b】（偏置量的符号：b一般是小写的）的新参数，有了偏置量，可以多一个参数来左右一个神经元是否被激活，现在公式变成了这样：
如果：
（x1w1+x2w2+……xnwn）+ b>θ
则接收到这些信号的那个神经元会被激活。
如果：
（x1w1+x2w2+……xnwn）+ b<=θ
则接收这些信号的那个神经元不会被激活。

此时的感知机，因为可以输出两种情况，对应0,1（激活与不激活）
如果输入信号值只有两个——x1,x2，则可以将这些输出值分别在平面直角坐标轴中表示出来（如果输入维度太多，那么就不是平面直角坐标系可以表示的了）
凡是输出为0的情况就在一边，而输出为1的情况就在另一边，这样一来，就非常方便地画出一条【直线】将这两种情况一分为二，这就是原始感知机能够做到的神经网络效果了：
 

原始简单的感知机只能是这样一条直线的【线性处理】，如果面对下图的情况，0和1并不是完全分开在两边，原始简单的感知机就没有办法了：
 
这时，0和1已经混合在一起了，直线根本无法分割，而原始简单的感知机只能【线性】计算处理，就完全没有办法了。
于是，就会感知机引入了【激活函数】

【激活函数】
有了激活函数，可以将感知机【去线性化】，那么感知机将可以实现变【直线】分割为【曲线】分割，上面的图片在有了激活函数对输出结果进行处理后，就可以通过曲线来进行分割：
 
我的简单理解就是，通过激活函数可以使神经元的输出不再是只有0，1这样的简单情况，而是可以使原本原始感知只能划出的直线根据需要弯曲以更好的分类。
激活函数用在神经元输出结果后对结果进行再次处理。
 
最流行的激活函数类型
1.Sigmoid函数或者Logistic函数
2.Tanh — Hyperbolic tangent（双曲正切函数）
3.ReLu -Rectified linear units（线性修正单元）
tensorflow一共提供了七种不同的激活函数，而且还支持自定义激活函数。

【今天源代码完成的批注】
```
import tensorflow as tf
import input_data

# 超参数
learning_rate = 0.5
epochs = 10
batch_size = 100 #每次增加训练用的图片数量 

#获取图片数据集
mnist=input_data.read_data_sets("MNIST_data/",one_hot=True)
#定义一个点位节点变量表示输入层所需要的矩阵——就是一系列的训练图片集，因为每张图片的像素是28x28,所以每张图片的总像素为：784（这时每张图片都变成了一维向量，这个向量包含了784个像素的值），而二维矩阵的另一维（就是None表示那一维）是用来表示总共有多少张图片
#one_hot又称之为：独热，
'''
图像标签的独热表示
变量mnist.train.labels表示训练图像的标签，它的形状是（55000，10）。原始的图像标签是数字0-9，我们完全可以用一个数字来存储图像标签，但为什么这里每个训练标签是一个10维的向量呢？其实，这个10维的向量是原先类别号的独热（one-hot）表示。所谓独热表示，就是“一位有效编码”。我们用N维的向量来表示N个类别，每个类别占据独立的一位，任何时候独热表示中只有一位是“1”，其他都为“0”。我们可以从下面这个表格中理解独热表示：
原文：https://blog.csdn.net/ruanjianbu/article/details/87735057 
'''
x=tf.placeholder("float",[None,784])
#上一行代码中，“float”指明了此变量的数据类型，后面的列表中，指明了一个矩形区域，其中None表示任意数，在真正执行这个变量时，再设置也可。
#placeholder点位节点表示 一个等待输入的节点。
#下面是输出层所需要的点位节点，输出为one_hot模式，我的理解是，one_hot模式就是一个有十个元素的列表，每个元素都只能取0-9之间的一个整数值，用以表示输出结果（也就是神经网络猜测（预测）的结果）
y=tf.placeholder("float",[None,10])

#下面定义预设的隐藏层所需要的权重与偏置值的正态分布集合
w1=tf.Variable(tf.random_normal([784,300],stddev=0.03),name='w1')
# W是Softmax模型的参数，将一个784维的输入转换为一个300维的输出
b1=tf.Variable(tf.random_normal([300]),name="b1")
# b是又一个Softmax模型的参数，我们一般叫做“偏置项”（bias）。
#w1=tf.Variable(tf.zeros([784,300]),name='w1')
#b1=tf.Variable(tf.zeros([300]),name="b1")

#下面定义预设的输出层所需要的权重与偏置值
w2=tf.Variable(tf.random_normal([300,10],stddev=0.03),name='w2')
b2=tf.Variable(tf.random_normal([10]),name='b2')
'''
https://blog.csdn.net/dcrmg/article/details/79028043
tf.random_normal()函数用于从服从指定正太分布的数值中取出指定个数的值。
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
    shape: 输出张量的形状，必选
    mean: 正态分布的均值，默认为0
    stddev: 正态分布的标准差，默认为1.0
    dtype: 输出的类型，默认为tf.float32
    seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样
    name: 操作的名称
'''
#下面构造隐藏层网络，定义运算
hidden_out=tf.add(tf.matmul(x,w1),b1) #--hidden_out=wx+b
#这儿使用了激活函数relu，激活函数的作用是，让神经网络层的加权和计算的线性化消失，变成非线性化。
hidden_out=tf.nn.relu(hidden_out)
'''
relu运算：
https://www.cnblogs.com/xh_chiang/p/9132524.html
TF-激活函数 tf.nn.relu 介绍
tf.nn.relu(features, name = None)
 这个函数的作用是计算激活函数 relu，即 max(features, 0)。即将矩阵中每行的非最大值置0。
import tensorflow as tf
a = tf.constant([-1.0, 2.0])
with tf.Session() as sess:
    b = tf.nn.relu(a)
    print sess.run(b)
以上程序输出的结果是：[0. 2.]
'''

#构造输出层，定义运算
# y=softmax(Wx + b)，y表示模型的输出，使用了一种保证实际图像y_和y尽量相似的模型算法softmax（优化算法？）
y_ = tf.nn.softmax(tf.matmul(hidden_out, w2) + b2) #加权变换并进行softmax回归，得到预测概率
#上一句中，使用hiden_out作为公式中的x，因为输出层使用隐藏层的输出结果作为输入值
'''
《TensorFlow：实战Google深度学习框架（第二版）》p76
就是说，任意事件发生的概率都在0 和1 之间，且总有某一个事件发生（ 概率的和为1 ）。
如果将分类问题中“ 一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。
因为事件“ 一个样例属于不正确的类别”的概率为0,而“ 一个样例属于正确的类别”的概率为l 。
如何将神经网络前向传播得到的结果也变成概率分布呢？ 
Softmax 回归就是一个非常常用的方法。Softmax 回归本身可以作为一个学习算法来优化分类结果，但在TensorFlow 中，
 softmax回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出变成一个概率分布。
'''
#下面求交叉熵损失——通过交叉熵来计算【预测的概率分布】（上面的softmax回归函数已经将输出层的结果变成了概率分布结果）和【真实答案的概率分布】之间的距离了。（距离值越小，就说明预测得越准确）
y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped) + (1 - y) * tf.log(1 - y_clipped), axis=1))
'''
《TensorFlow：实战Google深度学习框架（第二版）》p77
reduce_mean就是计算两个概率分布之间交叉熵损失的函数。
通过tf.clip_by_value 函数可以将一个张量中的数值限制在一个范围之内，这样可以避免一些运算错误（比如logO 是无效的）。
下面给出了使用tf.clip_by_value 的简单样例：
v = tf.constant([[l.0, 2.0 , 3 .0], [4.0,5.0,6.0]))
print tf.clip_by_value(v, 2.5, 4.5).eval()
＃输出［［ 2 .5 2 . 5 3.] [ 4. 4 . 5 4.5]]
在以上样例中可以看到，小于2.5 的数都被换成了2.5 ，而大于4.5 的数都被换成了4.5 。
------------------------------------------------------------
至此，我们得到了两个重要的Tensor：y和y_。
y_是模型的输出，y是实际的图像标签，不要忘了y是独热表示的
下面我们就会根据y和y_构造损失
根据y, y_构造交叉熵损失
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_)))
原文：https://blog.csdn.net/ruanjianbu/article/details/87735057 
------------------------------------------------------------------------------
《TensorFlow：实战Google深度学习框架（第二版）》p76
交叉熵损失只能计算两个概念分布之间的相距值（相似度),因此之前要使用softmax这样的回归函数将结果概率化。
交叉熵函数不是对称的C H(p, q ):;: H(q,p ）） ，
它刻画的是通过概率分布q 来表达概率分布p 的困难程度。
因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时， p 代表的是正确答案， q 代表的是预测值。
交叉熵刻画的是两个概率分布的距离，也就是说交叉熵值越小， 两个概率分布越接近。
下面将给出两个具体样例来直观地说明通过交叉熵可以判断预测答案和真实答案之间的距离。
假设有一个三分类问题，某个样例的正确答案是（ l，0，0 ） 。
某模型经过Softmax 回归之后的预测答案是（ 0.5 ,0.4 ,0.1 ），那么这个预测和正确答案之间的交叉娟为：
H((1, 0, 0), (0.5, 0.4, 0.1)) ＝-(1x log0.5 + 0 x log0.4 + 0 x log0.1)≈0.3
如果另外一个模型的预测是（0.8,0.1 , 0. l ），那么这个预测值和真实值之间的交叉熵是：
H((1, 0, 0), (0.8, 0.1, 0.1)) ＝-(1x log0.8 + 0 x log0.1 + 0 x log0.1)≈0.1
从直观上可以很容易地知道第二个预测答案要优于第一个。通过交叉熵计算得到的结果也是一致的（第二个交叉熵的值更小）
这儿的log表示 Log函数，其实就是ln（表示自然数对数函数）
对数函数log,以e(2.71828)为底，则 log0.5表示 ，求2.71828的多少次方=0.5
'''

#tensorflow中有一个综合性的函数同时实现回归计算成概率分布后，再直接比较相似度：
'''
《TensorFlow：实战Google深度学习框架（第二版）》p78
因为交叉煽一般会与softmax 回归一起使用，所以TensorFlow 对这两个功能进行了统一封装，
并提供了tf.nn.softmax_cross_entropy_with_logits 函数。
比如可以直接通过以下代码来实现使用了softmax 回归之后的交叉崎损失函数：
Cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)
其中y_代表了原始神经网络的输出结果，而y给出了标准答案。
'''

#下面使用基础梯度下降算法构建一个优化器，
optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)
#定义初始化operation和准确率node
# 下面定义的是初始化所有变量的运算
init_op = tf.global_variables_initializer()

# 创建准确率节点
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# 开始会话进行训练
with tf.Session() as sess:
   # 下一句代码才真正执行了初始化所有变量的运算
   sess.run(init_op)
   total_batch = int(len(mnist.train.labels) / batch_size)
   for epoch in range(epochs):
        avg_cost = 0
        for i in range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)
            _,c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})
            avg_cost += c / total_batch
        print("Epoch:", (epoch + 1), "cost =", "{:.3f}".format(avg_cost))
   print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))
 ```
